{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e56fa6c8-5301-460e-936c-2c1303802ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors: Maria DaRocha (300399718), William Huang (300653623)\n",
    "# AIML427: Group Project (A3)\n",
    "\n",
    "# APACHE SPARK POST-PROCESSING CODE:\n",
    "# (Paired t-Test / Wilcoxon / Data Visualisation)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172ed152-16f3-412f-90d3-de946c8cfd9d",
   "metadata": {},
   "source": [
    "# Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaff53e2-8ae6-417a-806c-727787003698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN_ACCURACY\n",
      "  DT Mean: 0.9523, Std: 0.0007\n",
      "  LR Mean: 0.9301, Std: 0.0010\n",
      "  t-statistic: 63.9096, p-value: 0.000000\n",
      "  Wilcoxon p-value:     0.0020\n",
      "  Statistically significant? YES\n",
      "\n",
      "TRAIN_PRECISION\n",
      "  DT Mean: 0.9524, Std: 0.0007\n",
      "  LR Mean: 0.9307, Std: 0.0010\n",
      "  t-statistic: 62.7084, p-value: 0.000000\n",
      "  Wilcoxon p-value:     0.0020\n",
      "  Statistically significant? YES\n",
      "\n",
      "TRAIN_RECALL\n",
      "  DT Mean: 0.9523, Std: 0.0007\n",
      "  LR Mean: 0.9301, Std: 0.0010\n",
      "  t-statistic: 63.9096, p-value: 0.000000\n",
      "  Wilcoxon p-value:     0.0020\n",
      "  Statistically significant? YES\n",
      "\n",
      "TRAIN_F1\n",
      "  DT Mean: 0.9523, Std: 0.0007\n",
      "  LR Mean: 0.9301, Std: 0.0010\n",
      "  t-statistic: 63.8642, p-value: 0.000000\n",
      "  Wilcoxon p-value:     0.0020\n",
      "  Statistically significant? YES\n",
      "\n",
      "TRAIN_ROC_AUC\n",
      "  DT Mean: 0.9521, Std: 0.0020\n",
      "  LR Mean: 0.9806, Std: 0.0003\n",
      "  t-statistic: -49.8908, p-value: 0.000000\n",
      "  Wilcoxon p-value:     0.0020\n",
      "  Statistically significant? YES\n",
      "\n",
      "Plots saved as plot_<metric>.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from scipy.stats import ttest_rel, wilcoxon\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load and parse the results\n",
    "with open(\"AllResultsFinal\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Function to extract a metric from seed blocks\n",
    "def extract_seed_metric(metric_name):\n",
    "    data = []\n",
    "    model = None\n",
    "    seed = None\n",
    "    for line in lines:\n",
    "        if line.startswith(\"SEED\"):\n",
    "            match = re.search(r\"SEED (\\d+)\", line)\n",
    "            if match:\n",
    "                seed = int(match.group(1))\n",
    "                model = \"LR\" if \"LOGISTIC\" in line else \"DT\"\n",
    "        elif line.startswith(metric_name + \":\"):\n",
    "            try:\n",
    "                val = float(line.split(\":\")[1].strip())\n",
    "                data.append({\"seed\": seed, \"model\": model, \"metric\": metric_name, \"value\": val})\n",
    "            except (IndexError, ValueError):\n",
    "                continue\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Extract training metrics\n",
    "metrics = [\"train_accuracy\", \"train_precision\", \"train_recall\", \"train_f1\", \"train_roc_auc\"]\n",
    "frames = [extract_seed_metric(m) for m in metrics]\n",
    "df = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "# Plot each metric using box and strip plot\n",
    "sns.set(style=\"whitegrid\", context=\"notebook\", font_scale=1.1)\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df_metric = df[df.metric == metric]\n",
    "    ax = sns.boxplot(data=df_metric, x=\"model\", y=\"value\", palette=\"pastel\")\n",
    "    sns.stripplot(data=df_metric, x=\"model\", y=\"value\", color=\"black\", size=5, jitter=0.15, alpha=0.7)\n",
    "\n",
    "    plt.title(f\"Comparison of {metric.replace('_', ' ').title()} Across Models\")\n",
    "    plt.ylabel(metric.replace(\"_\", \" \").title())\n",
    "    plt.xlabel(\"Model\")\n",
    "    plt.ylim(0.8, 1.05)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"plot_{metric}.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# --- Mean Metrics ---\n",
    "\n",
    "# Compute means\n",
    "summary = df.groupby([\"model\", \"metric\"]).value.mean().unstack()\n",
    "categories = list(summary.columns)\n",
    "labels = list(summary.index)\n",
    "num_vars = len(categories)\n",
    "\n",
    "# Compute angles for radar\n",
    "angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
    "angles += angles[:1]\n",
    "\n",
    "# Create radar plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "ax = plt.subplot(111, polar=True)\n",
    "\n",
    "for label in labels:\n",
    "    values = summary.loc[label].tolist()\n",
    "    values += values[:1]\n",
    "    ax.plot(angles, values, label=label)\n",
    "    ax.fill(angles, values, alpha=0.1)\n",
    "\n",
    "ax.set_title(\"Average Training Metric Comparison (Radar Chart)\", size=15)\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels([m.replace(\"_\", \" \").title() for m in categories])\n",
    "ax.set_yticks([0.85, 0.9, 0.95, 1.0])\n",
    "ax.set_ylim(0.85, 1.0)\n",
    "plt.legend(loc=\"upper right\", bbox_to_anchor=(1.3, 1.1))\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"radar_train_metrics.png\")\n",
    "plt.close()\n",
    "\n",
    "# Perform and display significance tests\n",
    "for metric in metrics:\n",
    "    vals = extract_seed_metric(metric)\n",
    "    # Filter by model, sort by seed\n",
    "    dt_scores = vals[vals[\"model\"] == \"DT\"].sort_values(\"seed\")[\"value\"]\n",
    "    lr_scores = vals[vals[\"model\"] == \"LR\"].sort_values(\"seed\")[\"value\"]\n",
    "\n",
    "\n",
    "    # Paired t-Test\n",
    "    t_stat, p_val = ttest_rel(dt_scores, lr_scores)\n",
    "    # Also try Wilcoxon (non-parametric alternative)\n",
    "    #  Note: no (normality) assumption violation; Added only for assignment completeness\n",
    "    w_stat, p_val_w = wilcoxon(dt_scores, lr_scores)\n",
    "\n",
    "    print(f\"\\n{metric.upper()}\")\n",
    "    print(f\"  DT Mean: {dt_scores.mean():.4f}, Std: {dt_scores.std():.4f}\")\n",
    "    print(f\"  LR Mean: {lr_scores.mean():.4f}, Std: {lr_scores.std():.4f}\")\n",
    "    print(f\"  t-statistic: {t_stat:.4f}, p-value: {p_val:.6f}\")\n",
    "    print(f\"  Wilcoxon p-value:     {p_val_w:.4f}\")\n",
    "    print(\"  Statistically significant?\" + (\" YES\" if p_val < 0.05 else \" NO\"))\n",
    "\n",
    "print(\"\\nPlots saved as plot_<metric>.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c168d7fc-6fdd-488b-8f98-6188d96b180a",
   "metadata": {},
   "source": [
    "# Testing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "895befa4-05f0-4841-9483-cb29ac4c96f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST_ACCURACY\n",
      "  DT Mean: 0.9519, Std: 0.0015\n",
      "  LR Mean: 0.9297, Std: 0.0016\n",
      "  t-statistic: 33.9416, p-value: 0.000000\n",
      "  Wilcoxon p-value:     0.0020\n",
      "  Statistically significant? YES\n",
      "\n",
      "TEST_PRECISION\n",
      "  DT Mean: 0.9520, Std: 0.0015\n",
      "  LR Mean: 0.9304, Std: 0.0015\n",
      "  t-statistic: 32.9664, p-value: 0.000000\n",
      "  Wilcoxon p-value:     0.0020\n",
      "  Statistically significant? YES\n",
      "\n",
      "TEST_RECALL\n",
      "  DT Mean: 0.9519, Std: 0.0015\n",
      "  LR Mean: 0.9297, Std: 0.0016\n",
      "  t-statistic: 33.9416, p-value: 0.000000\n",
      "  Wilcoxon p-value:     0.0020\n",
      "  Statistically significant? YES\n",
      "\n",
      "TEST_F1\n",
      "  DT Mean: 0.9519, Std: 0.0015\n",
      "  LR Mean: 0.9297, Std: 0.0016\n",
      "  t-statistic: 33.8797, p-value: 0.000000\n",
      "  Wilcoxon p-value:     0.0020\n",
      "  Statistically significant? YES\n",
      "\n",
      "TEST_ROC_AUC\n",
      "  DT Mean: 0.9518, Std: 0.0015\n",
      "  LR Mean: 0.9806, Std: 0.0008\n",
      "  t-statistic: -45.3142, p-value: 0.000000\n",
      "  Wilcoxon p-value:     0.0020\n",
      "  Statistically significant? YES\n",
      "\n",
      "Plots saved as plot_<metric>.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from scipy.stats import ttest_rel, wilcoxon\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load and parse the results\n",
    "with open(\"AllResultsFinal\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Function to extract a metric from seed blocks\n",
    "def extract_seed_metric(metric_name):\n",
    "    data = []\n",
    "    model = None\n",
    "    seed = None\n",
    "    for line in lines:\n",
    "        if line.startswith(\"SEED\"):\n",
    "            match = re.search(r\"SEED (\\d+)\", line)\n",
    "            if match:\n",
    "                seed = int(match.group(1))\n",
    "                model = \"LR\" if \"LOGISTIC\" in line else \"DT\"\n",
    "        elif line.startswith(metric_name + \":\"):\n",
    "            try:\n",
    "                val = float(line.split(\":\")[1].strip())\n",
    "                data.append({\"seed\": seed, \"model\": model, \"metric\": metric_name, \"value\": val})\n",
    "            except (IndexError, ValueError):\n",
    "                continue\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Extract testing metrics\n",
    "metrics = [\"test_accuracy\", \"test_precision\", \"test_recall\", \"test_f1\", \"test_roc_auc\"]\n",
    "frames = [extract_seed_metric(m) for m in metrics]\n",
    "df = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "# Plot each metric using box and strip plot\n",
    "sns.set(style=\"whitegrid\", context=\"notebook\", font_scale=1.1)\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df_metric = df[df.metric == metric]\n",
    "    ax = sns.boxplot(data=df_metric, x=\"model\", y=\"value\", palette=\"pastel\")\n",
    "    sns.stripplot(data=df_metric, x=\"model\", y=\"value\", color=\"black\", size=5, jitter=0.15, alpha=0.7)\n",
    "\n",
    "    plt.title(f\"Comparison of {metric.replace('_', ' ').title()} Across Models\")\n",
    "    plt.ylabel(metric.replace(\"_\", \" \").title())\n",
    "    plt.xlabel(\"Model\")\n",
    "    plt.ylim(0.8, 1.05)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"plot_{metric}.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# --- Mean Metrics ---\n",
    "\n",
    "# Compute means\n",
    "summary = df.groupby([\"model\", \"metric\"]).value.mean().unstack()\n",
    "categories = list(summary.columns)\n",
    "labels = list(summary.index)\n",
    "num_vars = len(categories)\n",
    "\n",
    "# Compute angles for radar\n",
    "angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
    "angles += angles[:1]\n",
    "\n",
    "# Create radar plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "ax = plt.subplot(111, polar=True)\n",
    "\n",
    "for label in labels:\n",
    "    values = summary.loc[label].tolist()\n",
    "    values += values[:1]\n",
    "    ax.plot(angles, values, label=label)\n",
    "    ax.fill(angles, values, alpha=0.1)\n",
    "\n",
    "ax.set_title(\"Average Testing Metric Comparison (Radar Chart)\", size=15)\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels([m.replace(\"_\", \" \").title() for m in categories])\n",
    "ax.set_yticks([0.85, 0.9, 0.95, 1.0])\n",
    "ax.set_ylim(0.85, 1.0)\n",
    "plt.legend(loc=\"upper right\", bbox_to_anchor=(1.3, 1.1))\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"radar_test_metrics.png\")\n",
    "plt.close()\n",
    "\n",
    "# Perform and display significance tests\n",
    "for metric in metrics:\n",
    "    vals = extract_seed_metric(metric)\n",
    "    # Filter by model, sort by seed\n",
    "    dt_scores = vals[vals[\"model\"] == \"DT\"].sort_values(\"seed\")[\"value\"]\n",
    "    lr_scores = vals[vals[\"model\"] == \"LR\"].sort_values(\"seed\")[\"value\"]\n",
    "\n",
    "    # Paired t-Test\n",
    "    t_stat, p_val = ttest_rel(dt_scores, lr_scores)\n",
    "    # Also try Wilcoxon (non-parametric alternative)\n",
    "    #  Note: no (normality) assumption violation; Added only for assignment completeness\n",
    "    w_stat, p_val_w = wilcoxon(dt_scores, lr_scores)\n",
    "\n",
    "    print(f\"\\n{metric.upper()}\")\n",
    "    print(f\"  DT Mean: {dt_scores.mean():.4f}, Std: {dt_scores.std():.4f}\")\n",
    "    print(f\"  LR Mean: {lr_scores.mean():.4f}, Std: {lr_scores.std():.4f}\")\n",
    "    print(f\"  t-statistic: {t_stat:.4f}, p-value: {p_val:.6f}\")\n",
    "    print(f\"  Wilcoxon p-value:     {p_val_w:.4f}\")\n",
    "    print(\"  Statistically significant?\" + (\" YES\" if p_val < 0.05 else \" NO\"))\n",
    "\n",
    "print(\"\\nPlots saved as plot_<metric>.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "885355ff-43c2-4d1a-9019-805d52b745b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean Z-scores by Metric and Model:\n",
      "model               DT      LR\n",
      "metric                        \n",
      "test_accuracy   0.9913 -0.9913\n",
      "test_f1         0.9913 -0.9913\n",
      "test_precision  0.9914 -0.9914\n",
      "test_recall     0.9913 -0.9913\n",
      "test_roc_auc   -0.9968  0.9968\n",
      "\n",
      "Overall Mean Z-score per Model:\n",
      "model\n",
      "DT    0.5937\n",
      "LR   -0.5937\n",
      "Name: z_score, dtype: float64\n",
      "\n",
      "Plots saved as zscore_<metric>.png\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "# Compute z-scores\n",
    "# TEST only for brevity: (Other metrics sufficiently informative)\n",
    "df[\"z_score\"] = df.groupby(\"metric\")[\"value\"].transform(zscore)\n",
    "\n",
    "# Plot z-score distributions\n",
    "sns.set(style=\"whitegrid\", context=\"notebook\", font_scale=1.1)\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df_metric = df[df.metric == metric]\n",
    "    sns.violinplot(data=df_metric, x=\"model\", y=\"z_score\", inner=\"point\", palette=\"muted\")\n",
    "    plt.title(f\"Z-score Normalized Distribution of {metric.replace('_', ' ').title()}\")\n",
    "    plt.ylabel(\"Z-score\")\n",
    "    plt.xlabel(\"Model\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"zscore_{metric}.png\")\n",
    "    plt.close()\n",
    "\n",
    "mean_z = df.groupby([\"metric\", \"model\"])[\"z_score\"].mean().unstack()\n",
    "print(\"\\nMean Z-scores by Metric and Model:\")\n",
    "print(mean_z.round(4))\n",
    "\n",
    "overall_mean_z = df.groupby(\"model\")[\"z_score\"].mean()\n",
    "print(\"\\nOverall Mean Z-score per Model:\")\n",
    "print(overall_mean_z.round(4))\n",
    "\n",
    "print(\"\\nPlots saved as zscore_<metric>.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
